---
title: "Diving into Factorial Designs in Causal Inference"
author: "Jeffrey Huang"
date: "2025-02-27"
image: "image.jpg"
categories: [causal-inference, research]
---

One key paradigm in causal inference literature is the Rubin Causal Model (RCM), also known as the potential outcome framework. It poses that given an individual $Y_i$, we can define two theoretical variables $Y_i(1)$ and $Y_i(0)$, indicating whether  a unit received treatment (the treated) or did not (the control), respectively.

This is often defined with the following equation modeling these two states of the world:

$Y_i = D_i Y_i(1) + (1-D_i) Y_i(0)$

With $D_i={0, 1}$ functioning as an indicator for a binary treatment. And from there on our unit level treatment effect is the difference $Y_i(1)-Y_i(0)$ for each unit $i$. Herein lies what Rubin coins the "fundamental problem of causal inference", that we simply cannot observe the both states of the world, known as potential outcomes, at once.

Fortunately, this field has been studied enough that we have a set of assumptions (stable unit treatment value, conditional independence) and methods (regression, matching, difference in differences, etc) that allow us to handle a variety of scenarios, both in an experimental and observational setting. (I really recommend Scott Cunningham's [The Mixtape](https://mixtape.scunning.com/). It gives a solid rundown on most causal inference methods and I've been using it to get up to speed in this topic.)

But it's not always enough to draw the line at just *one* treatment effect; it's realistic to expect that we'll sometimes want to look at how multiple treatments will affect an outcome, or interactions between this treatments even. A scenario where there are interactions between multiple treatments is called a factorial design, and while there's a long history in studying causal methods in this space,it still maintains relevance, and there's a lot of exciting ground to break in research. 

### Paper Review
There are a couple of papers that provide a solid enough intuition to interact with the rest of the literature without making everything seem like arcane knowledge. There's enough twists and turns involved that I wouldn't call it a "gentle" introduction. But having only prior been familiar with the Rubin Causal Model, understanding that framework alone was enough for me to eventually get my feet wet with its factorial extension. 

One great resource is a 2018 paper from Dasgupta et al, which builds onto the existing potential outcomes model to create a model for $2^K$ factorial designs. Say we are given $K$ binary factors, each taking a value of 0 or 1. Then let $z$ be a treatment combination of those factors given by a $K$-dimensional vector of -1's and +1's to represent negative/positive levels (if this sounds confusing at first, I'll dive deeper in a bit). This then gives us $Z$, the set of $2^K$ treatment combinations. Similar to with the single treatment RCM, under the stable unit treatment value assumption (SUTVA) we assert that all the potential outcomes for a unit $Y_i$ are completely explained by its assigned treatment combinations $z$. 

In fact, many of the assumptions we would normally make for a single treatment RCM can be applied to the factorial framework. We still value the ignorability/unconfoundedness assumption stating that potential outcomes are independent of treatment assignment (in this instance, the assignment matrix $Z$) when conditioned on confounding covariates. We can even still use methods such as the propensity score. The potential outcome matrix itself $Y$ for $N$ units is a $N \times 2^K$ matrix, in order to account for outcomes under $2^K$ possible treatment combinations (more on what this looks like next).

Now back to this novel construction of the treatment variable $z$. The treatment matrix $Z$ is inherently $K$-dimensional in order to express all possible combinations of $K$ factors. In contrast, for a single treatment case just a single column would be enough. In a regular binary treatment case, we can simply get a difference in average outcomes between treatment and control groups. Deriving the average treatment effect:

$\bar{ATE} = \bar{Y(1)} - \bar{Y(0)}$

is a pretty standard estimate that gives us good options for inference under certain assumptions.

In a factorial scenario this becomes more complicated. If we were considering three factors, for example, then in order to get an estimate of the main effect of factor 1 we'd need to account for not only the per-unit state of factor 1 but also the states of the other factors 2 and 3, as well as the two and three-level interactions between the three factors. This makes the standard difference in averages method for single treatments infeasible; it isn't as expressive as we'd like it to be in a multi-treatment scenario.

This leads to an important definition: the factorial effect of a unit is the "difference in averages between one half of the potential outcomes and the other half". Dasgupta defines a factorial effect for a unit $i$ as a vector with dimension $2^K$ $g$ with one half of its elements $-1$ and the other half $+1$, representing inactive and active states of a factor respectively.

Let's get a deeper understanding of why this construction exists for the contrast vector $g$. It builds on the intuition that an estimated factorial effect is the "difference in averages between one half of the potential outcomes and the other half", which carries over from the standard single-factor RCM. Again, with a single treatment scenario all we'd have to do is consider observations marked with an arbitrary treatment assignment vector, bin them in like groups (treated inviduals $Y|D=1$ altogether, controls $Y|D=0$ altogether) and take a simple difference in averages. 

The $g$ vector simply extends this exercise: Recall that under Dasgupta's design there are $2^K$ potential treatment combinations, with each combination $z \in Z$ being a single treatment combination. The $2^K$ makes sense; in any scenario where we have $p$ variables with two possible states, the total number of possible combinations is just $2^p$. Each $g$ vector will thus hold $2^K$ values, with half being -1 and the other half being +1, very similar to what we have for a single binary treatment. We'd have $K$ such $g$ vectors to represent main factorial effects. Going back to the three factor example, we'd have $g_1 = {(-1, -1, -1, -1, 1, 1, 1, 1)}^{'}$, $g_2 = {(-1, -1, 1, 1, -1, -1, 1, 1)}^{'}$, and $g_3 = {(-1, 1, -1, 1, -1, 1, -1, 1)}^{'}$ representing the contrast vectors to compute the factorial main effects for factors 1, 2, and 3 respectively. We then take ${K\choose 2}$ more $g$ vectors that represent the 2-level interaction effects, which we can derive by element-wise multiplications of the main effects $g$ vectors. Within our 3-factor example, we'd get 3 more $g$ vectors, each with an even balance of half +1's and half -1's. We can then add ${K\choose 3}$ $g$ vectors representing the 3 level interactions. In our three factor scenario this is just a single vector. 

Note that after all of this, we've derived exactly $2^{K-1}$ $g$ vectors. In general, at the end of this process you'll get a vector representing the $K$-way interaction between all the factors, represented by $g_{J-1}$ where $J=2^{K}$, and so we have $J-1$ total individual $g$ vectors, each one of length $J$, altogether representing factorial main and interaction effects.

Finally, we can express the factorial effect for a unit $i$ defined by $g_j$ as a linear combination of $i$'s potential outcomes, denoted by $\tau_{ij}$ and expressed as:

$\tau_{ij} = 2^{-(K-1)} {g_j}^{'} Y_i$

And this can be used to derive the sample factorial estimate:

$\bar\tau_{.j} = \frac{1}{N}\sum_{i=1}^{N}\tau_{ij} = 2^{-(K-1)} {g_j}^{'} \bar Y$

And voila! We now have an extension of the Rubin Causal framework to estimate factorial effects as we would for a single treatment.

This is definitely an interesting topic. Performing causal inference on multiple treatments at once is necessary across various fields, but has gotten less attention, likely due to requiring a more rigorous setup and design process. But it's nonetheless a valuable tool, especially in instances where doing a case by case single treatment study isn't enought to reveal the full picture. Be on the lookout for more blog posts on thsi topic as I do more work and research on it.