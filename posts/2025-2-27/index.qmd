---
title: "Diving into Factorial Designs in Causal Inference"
author: "Jeffrey Huang"
date: "2025-02-27"
image: "image.jpg"
categories: [causal-inference, research]
---

One key paradigm in causal inference literature is the Rubin Causal Model (RCM), also known as the potential outcome framework. It poses that given an individual $Y_i$, we can define two theoretical variables $Y_i(1)$ and $Y_i(0)$, indicating whether  a unit received treatment (the treated) or did not (the control), respectively.

This is often defined with the following equation modeling these two states of the world:

$Y_i = D_i Y_i(1) + (1-D_i) Y_i(0)$

With $D_i={0, 1}$ functioning as an indicator for a binary treatment. And from there on our unit level treatment effect is the difference $Y_i(1)-Y_i(0)$ for each unit i. Herein lies what Rubin coins the "fundamental problem of causal inference", that we simply cannot observe the both states of the world, known as potential outcomes, at once.

Fortunately, this field has been studied enough that we have a set of assumptions (stable unit treatment value, conditional independence) and methods (regression, matching, difference in differences, etc) that allow us to handle a variety of scenarios, both in an experimental and observational setting. (I really recommend Scott Cunningham's [The Mixtape](https://mixtape.scunning.com/). it gives a solid rundown on most causal inference methods and I've been using it to get up to speed in this topic.)

But it's not always enough to draw the line at just *one* treatment effect; it's realistic to expect that we'll sometimes want to look at how multiple treatments will affect an outcome, or interactions between this treatments even. A scenario where there are interactions between multiple treatments is called a factorial design, and while there's a long history in studying causal methods in this space,it still maintains relevance, and there's a lot of exciting ground to break in research. 

### Paper Review
There are a couple of papers that provide a solid enough intuition to interact with the rest of the literature without making everything seem like arcane knowledge. I say this having only prior been familiar with the Rubin Causal Model. Understanding that framework was enough for me to get my feet wet with its factorial extension. 

One great resource is a 2018 paper from Dasgupta et al, which builds onto the existing potential outcomes model to create a model for $2^k$ factorial designs. Say we are given $K$ binary factors, each taking a value of 0 or 1. Then let $z$ be a treatment combination of those factors given by a $K$-dimensional vector of -1's and 1's to represent negative/positive levels (if this sounds confusing at first, I'll dive deeper in a bit). This then gives us $Z$, the set of $2^K$ treatment combinations. Similar to with the single treatment RCM, under the stable unit treatment value assumption (SUTVA) we assert that all the potential outcomes for a unit $Y_i$ are completely explained by its assigned treatment combinations $z$. 

In fact, many of the assumptions we would normally make for a single treatment RCM can be applied to the factorial framework. We still value the ignorability/unconfoundedness assumption stating that potential outcomes are independent of treatment assignment (in this instance, the assignment matrix Z) when conditioned on confounding covariates. We can still use methods such as the propensity score. The potential outcome matrix itself $Y$ for $N$ units is a $N \times 2^K$ matrix, in order to account for outcomes under $2^K$ possible treatment combinations (more on what this looks like next).

Now back to this novel construction of the treatment variable z. The treatment matrix Z is inherently K-dimensional, in order to express all possible combinations of K factors. In contrast, for a single treatment case just a single column would be enough. In a regular binary treatment case, we can simply get a difference in average outcomes between treatment and control groups. Deriving the average treatment effect $\bar{ATE} = \bar{Y(1)} - \bar{Y(0)}$ is a pretty standard estimate that gives us good options for inference under certain assumptions.

In a factorial scenario this becomes more complicated. If we were considering three factors, for example, then in order to get an estimate of the main effect of factor 1 we'd need to account for not only the per-unit state of factor 1 but also the states of the other factors 2 and 3, as well as the two and three-level interactions between the three factors. This makes the standard difference in averages method for single treatments infeasible; it isn't as expressive as we'd like it to be in a multi-treatment scenario.

This leads to an important definition: the factorial effect of a unit is the "difference in averages between one half of the potential outcomes and the other half". Dasgupta defines a factorial effect for a unit $i$ as a vector with dimension $2^K$ $g$ with one half of its elements $-1$ and the other half $+1$, representing inactive and active states of a factor respectively.

Let's get a deeper understanding of the construction of the contrast vector g. It builds on the intuition that an estimated factorial effect is the "difference in averages between one half of the potential outcomes and the other half", which carries over from the standard single-factor RCM. Again, with a single treatment scenario all we'd have to do is consider observations marked with an arbitrary treatment assignment vector, bin them in like groups (treated inviduals altogether, controls altogether) and take a simple difference in averages. The g vector simply extends this exercise: Recall that under Dasgupta's design there are $2^K$ potential treatment combinations, with each combination $z \in Z$ being a single treatment combination. The $2^K$ makes sense; in any scenario where we have $p$ variables with two possible states, the total number of possible combinations is just $2^p$.


