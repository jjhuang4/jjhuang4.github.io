[
  {
    "objectID": "posts/2025-1-20/index.html",
    "href": "posts/2025-1-20/index.html",
    "title": "Benchmarking Model Performance on Australian Electricity Demand Forecasting",
    "section": "",
    "text": "Forecasting electricity demand is an extremely relevant problem for energy providers, as it influences not only the cost of day to day operations but also the timeliness and effectiveness of preparing for extreme weather conditions. Additionally, providers have to account for variations in geographic locations, further increasing the complexity of forecasting tasks.\nIn this blog post, I aim to apply the Prophet forecasting model on the Australian New South Wales Electricity Market dataset, which features the electricity price and demand of New South Wales and Victoria, as well as the amount transferred between the two states.\nThe data contains 45,312 instances for the days from May 7, 1996 to December 5, 1998, with observations recorded every half hour in each 24 hour period. I account for this granularity with a multiple seasonality approach.\n\nelectricity.head()\n\n\n\n\n\n\n\n\ndate\nday\nperiod\nnswprice\nnswdemand\nvicprice\nvicdemand\ntransfer\nclass\n\n\n\n\n0\n0.0\n2\n0.000000\n0.056443\n0.439155\n0.003467\n0.422915\n0.414912\nUP\n\n\n1\n0.0\n2\n0.021277\n0.051699\n0.415055\n0.003467\n0.422915\n0.414912\nUP\n\n\n2\n0.0\n2\n0.042553\n0.051489\n0.385004\n0.003467\n0.422915\n0.414912\nUP\n\n\n3\n0.0\n2\n0.063830\n0.045485\n0.314639\n0.003467\n0.422915\n0.414912\nUP\n\n\n4\n0.0\n2\n0.085106\n0.042482\n0.251116\n0.003467\n0.422915\n0.414912\nDOWN\n\n\n\n\n\n\n\nThe data has a very fine level of granularity, making for a highly dense time series plot for the entire series. Right off the bat we can observe a longer term seasonality in the trend line of the graph.\n\nfig, ax = plot_series(electricity['nswdemand'])\nfig, ax = plot_series(electricity['nswprice'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting a couple smaller length sections of the series (in intervals of 1000 observations) lets us see smaller scale seasonalities, which are immediately discernable.\n\nfig, ax = plot_series(electricity['nswdemand'][0:1000])\n\n\n\n\n\n\n\n\n\nfig, ax = plot_series(electricity['nswdemand'][20000:21000])\n\n\n\n\n\n\n\n\n\nEDA\nThe magnitude of seasonal variations doesn’t appear to be influenced by the length of the time series, so I assume an additive decomposition to be the most appropriate such that\n\\(y_t = S_t + T_t + R_t\\)\nwhere data is decomposed into seasonal, trend, and remainder components respectively.\nThis data doesn’t contain any missing values:\n\n# Checking for missing values\nmissing_values = electricity.isna().sum()\nprint(missing_values)\n\ndate         0\nday          0\nperiod       0\nnswprice     0\nnswdemand    0\nvicprice     0\nvicdemand    0\ntransfer     0\nclass        0\ndtype: int64\n\n\nNow let’s perform an STL decomposition to get an idea of the seasonal and trend-cycle components of our data.\n\nstl = STLTransformer(return_components=True, sp=48)\ny_stl = stl.fit_transform(electricity['nswdemand'])\nplot_series(y_stl['transformed'][0:1000])\nplot_series(y_stl['seasonal'][0:1000])\nplot_series(y_stl['trend'][0:1000])\nplot_series(y_stl['resid'][0:1000])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerforming an STL transform with a periodicity of 48 (representing one day) eliminates the hourly seasonality from the time series, revealing the trend line as shown above, which appears to still contain weekly seasonality. We can make an educated guess that there’ll also be yearly seasonality, given the correlation between electricity demand and seasonal changes in weather. I’ll run STL transform again to extract out the weekly/yearly seasonalities.\n\nstl2 = STLTransformer(return_components=True, sp=48*7)\ny_stl2 = stl2.fit_transform(y_stl['trend'])\nplot_series(y_stl2['transformed'][0:17520])\nplot_series(y_stl2['seasonal'][0:2500])\nplot_series(y_stl2['trend'][0:17520])\nplot_series(y_stl2['resid'][0:17520])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt becomes more complicated to eyeball any obvious seasonalities from the trend cycle after extracting the weekly seasonalities. Some R packages such as feasts may provide more powerful tools for breaking down the short/long term seasonalities and features in the series, but for now this works as EDA.\nA closer look at the daily seasonality present in the data (2 periods, 1 periods plotted below):\n\nplot_series(electricity['nswdemand'][0:96])\nplot_series(electricity['nswdemand'][0:48])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProphet Model\nProphet is a model developed by Facebook for use in commercial forecasting applications; it has strong out of the box performance in data containing multiple seasonalities and holiday effects. I use a downgraded 1.1.5 version of Prophet due to optimization errors that arise with the 1.1.6 version.\nHyndman does a brief breakdown of the model and its structure. Prophet works as an additive model:\n\\(y(t) = g(t) + s(t) + h(t) + \\epsilon_t\\)\nWhich adds the trend \\(g(t)\\) as a piecewise linear model, the seasonality \\(s(t)\\) as Fourier terms, the holiday effects \\(h(t)\\) as dummy variables, and additional noise \\(\\epsilon_t\\). The model estimates changepoints and other structures through a Bayesian MCMC approach. Facebook’s Prophet has cmdstan as a dependency, but I’ve found a very detailed breakdown/implementation of the model using PyMC in this blog post.\nWhile the smallest level of granularity in some time series occurs through daily instances, this dataset has observations ever half-hour, meaning the period should be 48 for daily seasonality, 336 for weekly, and so on. We’ll have to add these as custom seasonalities to our Prophet model; this is accomplished by adding new custom seasonalities using the add_seasonality argument provided by the sktime wrapper.\nSince we’re using sktime, it has a slightly different way of handling add_seasonality() than in fbprophet; we have to add multiple custom seasonalities as a list of dictionaries to the constructor.\n\ny = electricity['nswdemand']\ny_train, y_test = temporal_train_test_split(y, test_size=0.2)\nforecaster = Prophet(\n    seasonality_mode='additive',\n    add_country_holidays={'country_name': 'Australia'},\n    yearly_seasonality=True,\n    weekly_seasonality=True,\n    daily_seasonality=True,\n    add_seasonality=[{\n        'name': 'daily-cycle',\n        'period': 48, # period of 48 representing observations per day\n        'fourier_order': 12\n    },\n    {\n        'name': 'weekly-cycle',\n        'period': 336, # period of 336 representing 7 day (weekly) cycle\n        'fourier_order': 3\n    },\n    {\n        'name': 'yearly-cycle',\n        'period': 17520,\n        'fourier_order': 10\n    }]\n)\nforecaster.fit(y_train)\n\nImporting plotly failed. Interactive plots will not work.\n19:47:37 - cmdstanpy - INFO - Chain [1] start processing\n19:48:04 - cmdstanpy - INFO - Chain [1] done processing\n\n\nProphet(add_country_holidays={'country_name': 'Australia'},\n        add_seasonality=[{'fourier_order': 12, 'name': 'daily-cycle',\n                          'period': 48},\n                         {'fourier_order': 3, 'name': 'weekly-cycle',\n                          'period': 336},\n                         {'fourier_order': 10, 'name': 'yearly-cycle',\n                          'period': 17520}],\n        daily_seasonality=True, weekly_seasonality=True,\n        yearly_seasonality=True)Please rerun this cell to show the HTML repr or trust the notebook.Prophet?Documentation for ProphetProphet(add_country_holidays={'country_name': 'Australia'},\n        add_seasonality=[{'fourier_order': 12, 'name': 'daily-cycle',\n                          'period': 48},\n                         {'fourier_order': 3, 'name': 'weekly-cycle',\n                          'period': 336},\n                         {'fourier_order': 10, 'name': 'yearly-cycle',\n                          'period': 17520}],\n        daily_seasonality=True, weekly_seasonality=True,\n        yearly_seasonality=True)\n\n\nWe then evaluate model performance with mean average percentage error, mean absolute error, and mean squared error.\n\n# Specify forecasting horizon and pass on to forecasting algorithm\ny_pred = forecaster.predict(np.arange(len(y_test)))\nmape = mean_absolute_percentage_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"MAPE: {mape} \\nMAE: {mae} \\nMSE: {mse}\")\n\nMAPE: 0.15347452360520786 \nMAE: 0.060155303375532855 \nMSE: 0.006071706056646205\n\n\nThese look like pretty decent results for a model that isn’t being exhaustively tuned/optimized (we could go on further with grid-search, cross validation, using other metrics like AIC or BIC, but this is already a solid benchmark for Prophet’s capabilities).\nLet’s visually see how our forecasts measure up to the actual values.\n\nfig, ax = plot_series(y_test, y_pred, labels=[\"y_test\", \"y_pred\"])\nfig, ax = plot_series(y_test[0:5000], y_pred[0:5000], labels=[\"y_test\", \"y_pred\"])\nfig, ax = plot_series(y_test[0:1000], y_pred[0:1000], labels=[\"y_test\", \"y_pred\"])\nfig, ax = plot_series(y_test[0:192], y_pred[0:192], labels=[\"y_test\", \"y_pred\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can plot the residuals to observe for any remaining seasonal patterns that weren’t captured by our model.\n\nfig, ax = plot_series(y_test[0:17520]-y_pred[0:17520])\nfig, ax = plot_series(y_test[0:1000]-y_pred[0:1000])\nfig, ax = plot_series(y_test[0:336]-y_pred[0:336])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen, we’ve captured a good deal of the daily, weekly, and yearly seasonalities from the original data from our forecast. However, there remains additional patterns in the data that we haven’t accounted for. Specifically, it appears that during the first half of the year -in the summer months- the energy demand spikes much more frequently than in the latter half, likely due to the use of air conditioning.\nThis shows how useful Prophet is in a situation where you’re able to clearly identify your seasonalities; without requiring any underlying knowledge of how the model works, it’s simple to fit a decent forecaster that captures most of the patterns in your data.\n\n\n\nMultiple Seasonal-Trend Forecasting\nOther methods are also just as performant. For example, the Multiple Seasonal-Trend Decomposition using Loess (MSTL) algorithm developed by Bandara, Hyndman, and Bergmeir handles multiple seasonal patterns well, which is exactly what our dataset exhibits. We saw above that I had to perform regular STL multiple times on the data because it exhibited multiple levels of seasonality at the daily, weekly, and yearly levels. With MSTL we still assume an additive decomposition, this time with the following structure:\n\\(y_t = S_t^{(1)} + S_t^{(2)} + ... + S_t^{(n)} + T_t + R_t\\)\nWith multiple terms for multiple seasonal components.\n\nmstl = MSTL(periods=[48, 48*7], return_components=True)\nmstl_components = mstl.fit_transform(y)\nprint(mstl_components.columns)\nplot_series(mstl_components['trend'][0:7500])\nplot_series(mstl_components['seasonal_48'][0:7500])\nplot_series(mstl_components['seasonal_336'][0:7500])\nplot_series(mstl_components['resid'][0:7500])\nplot_series(y[0:7500])\n\nIndex(['trend', 'resid', 'seasonal_48', 'seasonal_336'], dtype='object')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA closer look at the seasonal components for daily (period=48) and weekly (period=336) seasonalities:\n\nplot_series(mstl_components['seasonal_48'][0:1000])\nplot_series(mstl_components['seasonal_336'][0:1000])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese separate seasonal components reflecct the daily and weekly cycles of energy demand that I saw earlier using STL iteratively on the dataset. We can see how electricity demand falls at certain parts of the day and peaks in others. Additionally, we see how electricity demand falls in two days of the week (weekend) as opposed to working days. We can incorporate the results of the MSTL inside sktime’s StatsForecastMSTL class model, which interfaces with Nixtla’s implementation.\nI fit the forecaster with arguments for daily, weekly, and yearly seasonalities relative to the dataset:\n\nmodel = StatsForecastMSTL(season_length=[48, 48*7, 48*365])\nforecaster = model.fit(y=y_train)\n\nAnd now for evaluating model performance:\n\ny_pred = forecaster.predict(np.arange(len(y_test)))\nmape = mean_absolute_percentage_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"MAPE: {mape} \\nMAE: {mae} \\nMSE: {mse}\")\n\nMAPE: 0.1522244466461589 \nMAE: 0.0591753040252963 \nMSE: 0.005841216097518591\n\n\n\nfig, ax = plot_series(y_test, y_pred, labels=[\"y_test\", \"y_pred\"])\nfig, ax = plot_series(y_test[0:5000], y_pred[0:5000], labels=[\"y_test\", \"y_pred\"])\nfig, ax = plot_series(y_test[0:1000], y_pred[0:1000], labels=[\"y_test\", \"y_pred\"])\nfig, ax = plot_series(y_test[0:192], y_pred[0:192], labels=[\"y_test\", \"y_pred\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that forecasting using this method performs just as, if not slightly better than Prophet. In the background, the series is decomposed with MSTL and then the trend and seasonality are forecasted using various other models (can consult this documentation to find out more).\nWe’re able to capture the daily, weekly, and yearly seasonalities that we saw in previous stages inside this forecast, but one thing of note is that this method was incredibly inefficient on the dataset, which makes sense. Given seasonal periods of length 48, 336, and 17520 (representing daily, weekly, and yearly cycles) the model has to estimate many more coefficients and the overall fitting process is much more computationally expensive (it took me around 20 minutes to run the model fitting on my device).\nCompare this to the Prophet model, where we were able to fit the model in just 30 seconds by adding exogenous Fourier terms to account for the seasonalities, and it’s easy to see the tradeoffs."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Diving into Factorial Designs in Causal Inference\n\n\n\n\n\n\ncausal-inference\n\n\nresearch\n\n\n\n\n\n\n\n\n\nFeb 27, 2025\n\n\nJeffrey Huang\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking Model Performance on Australian Electricity Demand Forecasting\n\n\n\n\n\n\ntime-series\n\n\nforecasting\n\n\n\n\n\n\n\n\n\nJan 20, 2025\n\n\nJeffrey Huang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-2-27/index.html",
    "href": "posts/2025-2-27/index.html",
    "title": "Diving into Factorial Designs in Causal Inference",
    "section": "",
    "text": "One key paradigm in causal inference literature is the Rubin Causal Model (RCM), also known as the potential outcome framework. It poses that given an individual \\(Y_i\\), we can define two theoretical variables \\(Y_i(1)\\) and \\(Y_i(0)\\), indicating whether a unit received treatment (the treated) or did not (the control), respectively.\nThis is often defined with the following equation modeling these two states of the world:\n\\(Y_i = D_i Y_i(1) + (1-D_i) Y_i(0)\\)\nWith \\(D_i={0, 1}\\) functioning as an indicator for a binary treatment. And from there on our unit level treatment effect is the difference \\(Y_i(1)-Y_i(0)\\) for each unit \\(i\\). Herein lies what Rubin coins the “fundamental problem of causal inference”, that we simply cannot observe the both states of the world, known as potential outcomes, at once.\nFortunately, this field has been studied enough that we have a set of assumptions (stable unit treatment value, conditional independence) and methods (regression, matching, difference in differences, etc) that allow us to handle a variety of scenarios, both in an experimental and observational setting. (I really recommend Scott Cunningham’s The Mixtape. It gives a solid rundown on most causal inference methods and I’ve been using it to get up to speed in this topic.)\nBut it’s not always enough to draw the line at just one treatment effect; it’s realistic to expect that we’ll sometimes want to look at how multiple treatments will affect an outcome, or interactions between this treatments even. A scenario where there are interactions between multiple treatments is called a factorial design, and while there’s a long history in studying causal methods in this space,it still maintains relevance, and there’s a lot of exciting ground to break in research.\n\nPaper Review\nThere are a couple of papers that provide a solid enough intuition to interact with the rest of the literature without making everything seem like arcane knowledge. There’s enough twists and turns involved that I wouldn’t call it a “gentle” introduction. But having only prior been familiar with the Rubin Causal Model, understanding that framework alone was enough for me to eventually get my feet wet with its factorial extension.\nOne great resource is a 2018 paper from Dasgupta et al, which builds onto the existing potential outcomes model to create a model for \\(2^K\\) factorial designs. Say we are given \\(K\\) binary factors, each taking a value of 0 or 1. Then let \\(z\\) be a treatment combination of those factors given by a \\(K\\)-dimensional vector of -1’s and +1’s to represent negative/positive levels (if this sounds confusing at first, I’ll dive deeper in a bit). This then gives us \\(Z\\), the set of \\(2^K\\) treatment combinations. Similar to with the single treatment RCM, under the stable unit treatment value assumption (SUTVA) we assert that all the potential outcomes for a unit \\(Y_i\\) are completely explained by its assigned treatment combinations \\(z\\).\nIn fact, many of the assumptions we would normally make for a single treatment RCM can be applied to the factorial framework. We still value the ignorability/unconfoundedness assumption stating that potential outcomes are independent of treatment assignment (in this instance, the assignment matrix \\(Z\\)) when conditioned on confounding covariates. We can even still use methods such as the propensity score. The potential outcome matrix itself \\(Y\\) for \\(N\\) units is a \\(N \\times 2^K\\) matrix, in order to account for outcomes under \\(2^K\\) possible treatment combinations (more on what this looks like next).\nNow back to this novel construction of the treatment variable \\(z\\). The treatment matrix \\(Z\\) is inherently \\(K\\)-dimensional in order to express all possible combinations of \\(K\\) factors. In contrast, for a single treatment case just a single column would be enough. In a regular binary treatment case, we can simply get a difference in average outcomes between treatment and control groups. Deriving the average treatment effect:\n\\(\\bar{ATE} = \\bar{Y(1)} - \\bar{Y(0)}\\)\nis a pretty standard estimate that gives us good options for inference under certain assumptions.\nIn a factorial scenario this becomes more complicated. If we were considering three factors, for example, then in order to get an estimate of the main effect of factor 1 we’d need to account for not only the per-unit state of factor 1 but also the states of the other factors 2 and 3, as well as the two and three-level interactions between the three factors. This makes the standard difference in averages method for single treatments infeasible; it isn’t as expressive as we’d like it to be in a multi-treatment scenario.\nThis leads to an important definition: the factorial effect of a unit is the “difference in averages between one half of the potential outcomes and the other half”. Dasgupta defines a factorial effect for a unit \\(i\\) as a vector with dimension \\(2^K\\) \\(g\\) with one half of its elements \\(-1\\) and the other half \\(+1\\), representing inactive and active states of a factor respectively.\nLet’s get a deeper understanding of why this construction exists for the contrast vector \\(g\\). It builds on the intuition that an estimated factorial effect is the “difference in averages between one half of the potential outcomes and the other half”, which carries over from the standard single-factor RCM. Again, with a single treatment scenario all we’d have to do is consider observations marked with an arbitrary treatment assignment vector, bin them in like groups (treated inviduals \\(Y|D=1\\) altogether, controls \\(Y|D=0\\) altogether) and take a simple difference in averages.\nThe \\(g\\) vector simply extends this exercise: Recall that under Dasgupta’s design there are \\(2^K\\) potential treatment combinations, with each combination \\(z \\in Z\\) being a single treatment combination. The \\(2^K\\) makes sense; in any scenario where we have \\(p\\) variables with two possible states, the total number of possible combinations is just \\(2^p\\). Each \\(g\\) vector will thus hold \\(2^K\\) values, with half being -1 and the other half being +1, very similar to what we have for a single binary treatment. We’d have \\(K\\) such \\(g\\) vectors to represent main factorial effects. Going back to the three factor example, we’d have \\(g_1 = {(-1, -1, -1, -1, 1, 1, 1, 1)}^{'}\\), \\(g_2 = {(-1, -1, 1, 1, -1, -1, 1, 1)}^{'}\\), and \\(g_3 = {(-1, 1, -1, 1, -1, 1, -1, 1)}^{'}\\) representing the contrast vectors to compute the factorial main effects for factors 1, 2, and 3 respectively. We then take \\({K\\choose 2}\\) more \\(g\\) vectors that represent the 2-level interaction effects, which we can derive by element-wise multiplications of the main effects \\(g\\) vectors. Within our 3-factor example, we’d get 3 more \\(g\\) vectors, each with an even balance of half +1’s and half -1’s. We can then add \\({K\\choose 3}\\) \\(g\\) vectors representing the 3 level interactions. In our three factor scenario this is just a single vector.\nNote that after all of this, we’ve derived exactly \\(2^{K-1}\\) \\(g\\) vectors. In general, at the end of this process you’ll get a vector representing the \\(K\\)-way interaction between all the factors, represented by \\(g_{J-1}\\) where \\(J=2^{K}\\), and so we have \\(J-1\\) total individual \\(g\\) vectors, each one of length \\(J\\), altogether representing factorial main and interaction effects.\nFinally, we can express the factorial effect for a unit \\(i\\) defined by \\(g_j\\) as a linear combination of \\(i\\)’s potential outcomes, denoted by \\(\\tau_ij\\) and expressed as:\n\\(\\tau_ij = 2^{-(K-1)} {g_j}^{'} Y_i\\)\nAnd this can be used to derive the sample factorial estimate:\n\\(\\bar\\tau_.j = \\frac{1}{N}\\sum_{i=1}^{N}\\tau_ij = 2^{-(K-1)} {g_j}^{'} \\bar Y\\)\nAnd voila! We now have an extension of the Rubin Causal framework to estimate factorial effects as we would for a single treatment.\nThis is definitely an interesting topic. Performing causal inference on multiple treatments at once is necessary across various fields, but has gotten less attention, likely due to requiring a more rigorous setup and design process. But it’s nonetheless a valuable tool, especially in instances where doing a case by case single treatment study isn’t enought to reveal the full picture. Be on the lookout for more blog posts on thsi topic as I do more work and research on it."
  }
]